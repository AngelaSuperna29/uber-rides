# -*- coding: utf-8 -*-
"""uber bda proj .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CnjNjVWytwwQVUpuYjYZDhSaq2_6IA9p
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Initialize Spark session
spark = SparkSession.builder.appName("RideShareAnalysis").getOrCreate()

# Load dataset into an RDD
file_path = "/content/rideshare_kaggle.csv"
rdd = spark.sparkContext.textFile(file_path)

# Display first few rows
rdd.take(5)

# Transformations
header = rdd.first()
data_rdd = rdd.filter(lambda row: row != header)  # Remove header
split_rdd = data_rdd.map(lambda row: row.split(","))  # Split columns

# Filter Example: Keep only rows where the first column is not empty
filtered_rdd = split_rdd.filter(lambda row: row[0] != "")

# Map Example: Extract a specific column (e.g., second column)
column_rdd = split_rdd.map(lambda row: row[1])

# FlatMap Example: Flatten a list of values (split words)
flat_rdd = rdd.flatMap(lambda row: row.split(" "))

# Distinct Example: Get unique rows
distinct_rdd = split_rdd.distinct()

# ReduceByKey Example: Count occurrences of a column (e.g., second column)
pair_rdd = split_rdd.map(lambda row: (row[1], 1))
count_rdd = pair_rdd.reduceByKey(lambda a, b: a + b)

# GroupByKey Example
grouped_rdd = pair_rdd.groupByKey().mapValues(list)

# SortBy Example: Sort by second column
sorted_rdd = split_rdd.sortBy(lambda row: row[1])

# Actions
first_row = split_rdd.first()
row_count = split_rdd.count()
top_5 = split_rdd.take(5)
collected_data = split_rdd.collect()

# Show results
show_rdd("Filtered RDD", filtered_rdd)

show_rdd("Mapped Column RDD", column_rdd)

show_rdd("Flat Mapped RDD", flat_rdd)

show_rdd("Reduced By Key RDD", count_rdd)

show_rdd("Grouped By Key RDD", grouped_rdd)

show_rdd("Sorted RDD", sorted_rdd)

print("\nFirst Row:", first_row)

print("Row Count:", row_count)

# Filter rides with distance greater than 10 miles
filtered_rdd = data_rdd.filter(lambda row: float(row[4]) > 10 if row[4] else False)

# Count number of records
print(f"Total records: {data_rdd.count()}")

# Extracting specific columns (assuming relevant columns exist)
# Example: Extracting 'price' column
prices_rdd = data_rdd.map(lambda row: float(row[3]) if row[3] else 0)
print(f"Average Price: {prices_rdd.mean()}")

# Convert RDD to DataFrame for EDA
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Display schema
df.printSchema()

# Check for missing values in columns that can contain nulls
# Exclude columns with data types that don't support nulls (e.g., numeric types)
nullable_columns = [c for c in df.columns if dict(df.dtypes)[c] not in ['int', 'double', 'float', 'bigint', 'smallint', 'tinyint']]
missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in nullable_columns])
missing_values.show()

# Convert to Pandas for visualization
pandas_df = df.toPandas()

# Distribution of prices
plt.figure(figsize=(10, 5))
sns.histplot(pandas_df['price'].dropna(), bins=50, kde=True)
plt.title("Price Distribution")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()

# Predictive Modeling
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# Select features and target variable
feature_cols = ['distance', 'surge_multiplier']  # Adjust based on dataset
assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')
df_transformed = assembler.transform(df)

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.sql.types import FloatType
from pyspark.ml.feature import Imputer

# Imputation using mean (you can also use median or other strategies)
imputer = Imputer(
    inputCols=['price'],
    outputCols=['price_imputed']
).setStrategy("mean")

# Fit the imputer on your training data
imputer_model = imputer.fit(train_data)

# Transform the training data to replace missing values
train_data = imputer_model.transform(train_data).drop("price").withColumnRenamed("price_imputed", "price")

# Now, train the linear regression model
lr = LinearRegression(featuresCol='features', labelCol='price')
model = lr.fit(train_data)

# Train-test split
train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)

# Model evaluation
predictions = model.transform(test_data)
predictions.select("price", "prediction").show(10)

# Visualization of actual vs. predicted values
pandas_pred = predictions.select("price", "prediction").toPandas()
plt.figure(figsize=(10, 5))
sns.scatterplot(x=pandas_pred['price'], y=pandas_pred['prediction'])
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs. Predicted Prices")
plt.show()